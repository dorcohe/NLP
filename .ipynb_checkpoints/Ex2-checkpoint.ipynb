{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Ex2 - NLP"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nnltk.download(\"treebank\")\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package treebank to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/treebank.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import Counter, defaultdict\nimport random\nimport numpy as np\nimport operator\nfrom nltk.corpus import treebank\nlen(treebank.tagged_sents())\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "3914"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_data = treebank.tagged_sents()[:3000]\ntest_data = treebank.tagged_sents()[3000:]\nprint(train_data[0])",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## simple tager"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class simple_tagger:\n    def __init__(self):\n        self.map = {}\n        self.pos_list = []\n        self.words_result = 0\n        self.sentences_result = 0\n    \n    def train(self, data):\n        corpus_counts = defaultdict(Counter)\n        for d in data:\n            for word, pos in d:\n                if pos not in self.pos_list:\n                    self.pos_list.append(pos)\n                corpus_counts[word][pos] +=1\n        for word in corpus_counts:\n            # takefind the most common pos, if there is tie - the tie breaker is random\n            l = corpus_counts[word].most_common(1000)\n            options = [l[0][0]]\n            j=1\n            while len(l)>j and l[j][1] == l[j-1][1]:\n                options.append(l[j][0])\n                j += 1\n            value = random.choice(options)\n            self.map[word] = value\n            \n        \n    def evaluate(self, data):\n        count = 0\n        words_success = 0\n        sentences_success = 0\n        for d in data:\n            sentence_flag = True\n            for word, pos in d:\n                count += 1\n                if word in self.map:\n                    suggested_pos = self.map[word]\n                else:\n                    suggested_pos = random.sample(self.pos_list, k=1)[0]\n                if suggested_pos == pos:\n                    words_success += 1\n                else:\n                    sentence_flag = False\n            if sentence_flag:\n                sentences_success += 1\n        self.words_result = words_success/count # word accuracy \n        self.sentences_result = sentences_success/len(data) # sentence accuracy\n                \n        \nc = simple_tagger()\nc.train(train_data)\nc.evaluate(test_data)\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Simple tagger word result: \" + str(c.words_result))\nprint(\"Simple tagger sentences result: \" + str(c.sentences_result))\n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Simple tagger word result: 0.8598748111374919\nSimple tagger sentences result:0.06892778993435449\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## HMM tagger"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class hmm_tagger:\n    def __init__(self):\n        self.pos_dict = {}\n        self.words_dict = {}\n        self.A = None\n        self.B = None\n        self.PI = None\n        self.words_result = 0\n        self.sentences_result = 0\n        \n    def get_b_matrix(self, corpus_counts, pos_count):\n        B = np.zeros((len(self.pos_dict), len(self.words_dict)), dtype=float)\n        for word in corpus_counts:\n            word_index = self.words_dict[word]\n            for pos in corpus_counts[word]:\n                pos_index = self.pos_dict[pos]\n                B[pos_index][word_index] = corpus_counts[word][pos]/pos_count[pos]\n        return B\n    \n    def get_pi_matrix(self, pos_count):\n        PI = np.zeros(len(self.pos_dict), dtype=float) # intilazie pi vector\n        for pos in pos_count:\n            PI[self.pos_dict[pos]] = pos_count[pos]\n        PI = PI/sum(pos_count.values()) \n        return PI\n    \n    def get_a_matrix(self, data):\n        A = np.zeros((len(self.pos_dict),len(self.pos_dict)), dtype=float) # intilazie A Matrix\n        pos_counter = np.zeros(len(self.pos_dict))\n        for line in data:\n            last_pos = \"\"\n            for word, pos in line:\n                if (last_pos == \"\"):\n                    last_pos = pos\n                    continue\n                A[self.pos_dict[last_pos]][self.pos_dict[pos]] += 1\n                pos_counter[self.pos_dict[last_pos]] += 1\n                last_pos = pos\n        for pos, pos_index in self.pos_dict.items():\n            A[pos_index] = A[pos_index]/pos_counter[pos_index] #normalize A\n        return A\n            \n        \n    def train(self, data): #train HMM \n        corpus_counts = defaultdict(Counter)\n        pos_count = {}\n        word_index = 0\n        pos_index = 0\n        for d in data:\n            for word, pos in d:\n                if pos not in self.pos_dict:\n                    self.pos_dict[pos] = pos_index\n                    pos_count[pos] = 0\n                    pos_index = pos_index + 1 \n                if word not in self.words_dict:\n                    self.words_dict[word] = word_index\n                    word_index = word_index + 1\n                corpus_counts[word][pos] +=1\n                pos_count[pos] += 1\n        self.B = self.get_b_matrix(corpus_counts, pos_count)\n        self.PI = self.get_pi_matrix(pos_count)\n        self.A = self.get_a_matrix(data)\n        \n    def viterbi(self,word_list, A, B, Pi): #evlautate HMM\n        # initialization\n        T = len(word_list)\n        N = A.shape[0] # number of tags\n\n        delta_table = np.zeros((N, T)) # initialise delta table\n        psi = np.zeros((N, T))  # initialise the best path table\n\n        delta_table[:,0] = B[:, word_list[0]] * Pi\n\n        for t in range(1, T):\n            for s in range (0, N):\n                trans_p = delta_table[:, t-1] * A[:, s]\n                psi[s][t], delta_table[s][ t] = max(enumerate(trans_p), key=operator.itemgetter(1))\n                delta_table[s][t] = delta_table[s][t] * B[s][word_list[t]]\n\n        # Back tracking\n        seq = np.zeros(T)\n        seq[T-1] = delta_table[:, T-1].argmax()\n        for t in range(T-1, 0, -1):\n            seq[t-1] = psi[int(seq[t])][t]\n\n        return seq\n    \n    def evaluate(self, data):\n        sentence_success = 0\n        sentence_counter = 0\n        word_counter = 0 \n        word_success = 0\n        for line in data:\n            list_of_pos = []\n            list_of_words = []\n            seq = None\n            for word, pos in line:\n                if word in self.words_dict:\n                    list_of_words.append(self.words_dict[word])\n                else:\n                    list_of_words.append(np.random.choice(list(self.words_dict.values())))\n                    if len(list_of_words)>0:\n                        if seq is not None:\n                            seq = np.append(seq, self.viterbi(list_of_words, self.A, self.B, self.PI))\n                        else:\n                            seq = self.viterbi(list_of_words, self.A, self.B, self.PI)\n                    list_of_words = []\n            if (seq is not None and len(list_of_words)>0):\n                seq = np.append(seq, self.viterbi(list_of_words, self.A, self.B, self.PI))\n            elif len(list_of_words)>0:\n                seq = self.viterbi(list_of_words, self.A, self.B, self.PI)    \n            sentences_success_flag = True\n            index = 0\n            for word, pos in line:\n                word_counter += 1\n                if int(self.pos_dict[pos]) == int(seq[index]):\n                    word_success += 1\n                else:\n                    sentences_success_flag = False\n                index += 1\n            if sentences_success_flag:\n                sentence_success += 1\n        self.words_result = word_success/word_counter # word accuracy \n        self.sentences_result = sentence_success/len(data) # sentence accuracy    \n                                    \nhmm = hmm_tagger()\nhmm.train(train_data)\n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hmm.evaluate(test_data)\nprint(\"HMM tagger word result: \" + str(hmm.words_result))\nprint(\"HMM tagger sentences result: \" + str(hmm.sentences_result))",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "HMM tagger word result: 0.8682926829268293\nHMM tagger sentences result: 0.10393873085339168\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## results"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tag import tnt\ntnt_pos_tagger = tnt.TnT()\ntnt_pos_tagger.train(train_data)\nprint(\"MEMM tagger tagger word result: \"+ str(tnt_pos_tagger.evaluate(test_data)))\ncounter=0\nfor line in test_data:\n    if tnt_pos_tagger.evaluate([line])>0.95:\n        counter += 1\nprint(\"MEMM tagger tagger sentences result: \"+ str(counter/len(test_data)))\n\n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "MEMM tagger tagger word result: 0.875545003237643\nMEMM tagger tagger sentences result: 0.17067833698030635\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "EOF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}