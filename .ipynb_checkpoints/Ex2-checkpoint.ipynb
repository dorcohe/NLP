{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Ex2 - NLP"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nnltk.download(\"treebank\")\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package treebank to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/treebank.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import Counter, defaultdict\nimport random\nimport numpy as np\nfrom nltk.corpus import treebank\nlen(treebank.tagged_sents())\n",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "3914"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_data = treebank.tagged_sents()[:3000]\ntest_data = treebank.tagged_sents()[3000:]\nprint(train_data[0])",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## simple tager"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class simple_tagger:\n    def __init__(self):\n        self.map = {}\n        self.pos_list = []\n        self.words_result = 0\n        self.sentences_result = 0\n    \n    def train(self, data):\n        corpus_counts = defaultdict(Counter)\n        for d in data:\n            for word, pos in d:\n                if pos not in self.pos_list:\n                    self.pos_list.append(pos)\n                corpus_counts[word][pos] +=1\n        for word in corpus_counts:\n            # takefind the most common pos, if there is tie - the tie breaker is random\n            l = corpus_counts[word].most_common(1000)\n            options = [l[0][0]]\n            j=1\n            while len(l)>j and l[j][1] == l[j-1][1]:\n                options.append(l[j][0])\n                j += 1\n            value = random.choice(options)\n            self.map[word] = value\n            \n        \n    def evaluate(self, data):\n        count = 0\n        words_success = 0\n        sentences_success = 0\n        for d in data:\n            sentence_flag = True\n            for word, pos in d:\n                count += 1\n                if word in self.map:\n                    suggested_pos = self.map[word]\n                else:\n                    suggested_pos = random.sample(self.pos_list, k=1)[0]\n                if suggested_pos == pos:\n                    words_success += 1\n                else:\n                    sentence_flag = False\n            if sentence_flag:\n                sentences_success += 1\n        self.words_result = words_success/count\n        self.sentences_result = sentences_success/len(data)\n                \n        \nc = simple_tagger()\nc.train(train_data)\nc.evaluate(test_data)\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Simple tagger word result: \" + str(c.words_result))\nprint(\"Simple tagger sentences result: \" + str(c.sentences_result))\n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Simple tagger word result: 0.8598748111374919\nSimple tagger sentences result:0.06892778993435449\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## HMM tagger"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class hmm_tagger:\n    def __init__(self):\n        self.pos_dict = {}\n        self.words_dict = {}\n        \n    def get_b_matrix(self, corpus_counts):\n        B = np.zeros((len(self.words_dict), len(self.pos_dict)), dtype=float)\n        for word in corpus_counts:\n            word_index = self.words_dict[word]\n            total = 0\n            for pos in corpus_counts[word]:\n                total += corpus_counts[word][pos]\n            for pos in corpus_counts[word]:\n                pos_index = self.pos_dict[pos]\n                B[word_index][pos_index] = corpus_counts[word][pos]/total\n        return B\n    \n    def get_pi_matrix(self, data):\n        PI = np.zeros(len(self.pos_dict)) \n        for line in data:\n            word, pos = line[0]\n            pos_index = self.pos_dict[pos]\n            PI[pos_index] +=1\n        PI = PI/len(data)\n        return PI\n    \n    def get_a_matrix(self, data):\n        A = np.zeros((len(self.pos_dict),len(self.pos_dict))) \n        for line in data:\n            word, pos = line[0]\n            pos_index = self.pos_dict[pos]\n            PI[pos_index] +=1\n        PI = PI/len(data)\n        return PI\n            \n        \n    def train(self, data):\n        corpus_counts = defaultdict(Counter)\n        word_index = 0\n        pos_index = 0\n        for d in data:\n            for word, pos in d:\n                if pos not in self.pos_dict:\n                    self.pos_dict[pos] = pos_index\n                    pos_index = pos_index + 1 \n                if word not in self.words_dict:\n                    self.words_dict[word] = word_index\n                    word_index = word_index + 1\n                corpus_counts[word][pos] +=1\n        B = self.get_b_matrix(corpus_counts)\n        PI = self.get_pi_matrix(data)\n        \n    \n    def evaluate(self, data):\n        print(\"dor\")\nhmm = hmm_tagger()\nhmm.train(train_data)",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.17666667 0.         0.00866667 0.04266667 0.03633333 0.00033333\n 0.00066667 0.228      0.03666667 0.13266667 0.         0.003\n 0.00333333 0.05566667 0.         0.00133333 0.02433333 0.04733333\n 0.00166667 0.071      0.00066667 0.00066667 0.         0.\n 0.00766667 0.00133333 0.         0.08766667 0.00533333 0.00033333\n 0.00466667 0.00266667 0.003      0.007      0.00166667 0.00166667\n 0.         0.002      0.         0.00033333 0.00033333 0.\n 0.00033333 0.         0.00233333 0.        ]\n0.9999999999999998\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}