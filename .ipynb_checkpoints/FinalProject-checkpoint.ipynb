{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# NLP Final project\n\n## Ofer Lipman & Dor Cohen"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# pip install pytorch-pretrained-bert\nimport pandas as pd\nimport numpy as np\nimport csv\nimport os\nimport sys\nimport logging\nimport torch\nimport pickle\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom tqdm import tqdm_notebook, trange\nimport os\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n\nfrom multiprocessing import Pool, cpu_count\nfrom tools import *\nimport convert_examples_to_features\n\n\n",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_pretrained_bert'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-ba60d70ccd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarmupLinearSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# The input data dir. Should contain the .tsv files (or other data files) for the task.\nDATA_DIR = \"Data/\"",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "emobank_df = pd.read_csv('emobank.csv', header=None)\nemobank_df = emobank_df.iloc[1:]\nmsk = np.random.rand(len(emobank_df)) < 0.8\ntrain_df = emobank_df[msk]\ntest_df = emobank_df[~msk]",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df_bert = pd.DataFrame({\n    'id':range(len(train_df)),\n    'label':train_df[1],\n    'alpha':['a']*train_df.shape[0],\n    'text': train_df[4].replace(r'\\n', ' ', regex=True)\n})\n\ntrain_df_bert.head()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>alpha</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2.8</td>\n      <td>a</td>\n      <td>If I wasn't working here.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3.0</td>\n      <td>a</td>\n      <td>..\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>3.44</td>\n      <td>a</td>\n      <td>Goodwill helps people get off of public assist...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>3.6</td>\n      <td>a</td>\n      <td>Coming to Goodwill was the first step toward m...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4</td>\n      <td>3.0</td>\n      <td>a</td>\n      <td>I am now... totally off of welfare.\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   id label alpha                                               text\n2   0   2.8     a                          If I wasn't working here.\n3   1   3.0     a                                                ..\"\n4   2  3.44     a  Goodwill helps people get off of public assist...\n6   3   3.6     a  Coming to Goodwill was the first step toward m...\n7   4   3.0     a               I am now... totally off of welfare.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_df_bert = pd.DataFrame({\n    'id':range(len(test_df)),\n    'label':test_df[1],\n    'alpha':['a']*test_df.shape[0],\n    'text': test_df[4].replace(r'\\n', ' ', regex=True)\n})\n\ntest_df_bert.head()",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>alpha</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>a</td>\n      <td>Remember what she said in my last letter? \"</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>3.55</td>\n      <td>a</td>\n      <td>Sherry learned through our Future Works class ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2</td>\n      <td>2.7</td>\n      <td>a</td>\n      <td>I wanted to be there.</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3</td>\n      <td>3.37</td>\n      <td>a</td>\n      <td>In addition to that, by helping them find jobs...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>4</td>\n      <td>3.4</td>\n      <td>a</td>\n      <td>The business community welcomes not only anoth...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "    id label alpha                                               text\n1    0   3.0     a        Remember what she said in my last letter? \"\n5    1  3.55     a  Sherry learned through our Future Works class ...\n16   2   2.7     a                              I wanted to be there.\n29   3  3.37     a  In addition to that, by helping them find jobs...\n35   4   3.4     a  The business community welcomes not only anoth..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df_bert.to_csv(os.path.join(DATA_DIR, \"train.tsv\"), sep='\\t', index=False, header=False)\ntest_df_bert.to_csv(os.path.join(DATA_DIR, \"dev.tsv\"), sep='\\t', index=False, header=False)\n",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logger = logging.getLogger()\ncsv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text.\n\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass BinaryClassificationProcessor(DataProcessor):\n    \"\"\"Processor for binary classification dataset.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n    def get_labels(self):\n        \"\"\"See base class.\"\"\"\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef convert_example_to_feature(example_row):\n    # return example_row\n    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    tokens_b = None\n    if example.text_b:\n        tokens_b = tokenizer.tokenize(example.text_b)\n        # Modifies `tokens_a` and `tokens_b` in place so that the total\n        # length is less than the specified length.\n        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n    else:\n        # Account for [CLS] and [SEP] with \"- 2\"\n        if len(tokens_a) > max_seq_length - 2:\n            tokens_a = tokens_a[:(max_seq_length - 2)]\n\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n    segment_ids = [0] * len(tokens)\n\n    if tokens_b:\n        tokens += tokens_b + [\"[SEP]\"]\n        segment_ids += [1] * (len(tokens_b) + 1)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    if output_mode == \"classification\":\n        label_id = label_map[example.label]\n    elif output_mode == \"regression\":\n        label_id = float(example.label)\n    else:\n        raise KeyError(output_mode)\n\n    return InputFeatures(input_ids=input_ids,\n                         input_mask=input_mask,\n                         segment_ids=segment_ids,\n                         label_id=label_id)",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\n# Bert pre-trained model selected in the list: bert-base-uncased, \n# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n# bert-base-multilingual-cased, bert-base-chinese.\nBERT_MODEL = 'bert-base-cased'\n\n# The name of the task to train.I'm going to name this 'yelp'.\nTASK_NAME = 'yelp'\n\n# The output directory where the fine-tuned model and checkpoints will be written.\nOUTPUT_DIR = f'outputs/{TASK_NAME}/'\n\n# The directory where the evaluation reports will be written to.\nREPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n\n# This is where BERT will look for pre-trained models to load parameters from.\nCACHE_DIR = 'cache/'\n\n# The maximum total input sequence length after WordPiece tokenization.\n# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\nMAX_SEQ_LENGTH = 128\n\nTRAIN_BATCH_SIZE = 24\nEVAL_BATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 1\nRANDOM_SEED = 42\nGRADIENT_ACCUMULATION_STEPS = 1\nWARMUP_PROPORTION = 0.1\nOUTPUT_MODE = 'classification'\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "output_mode = OUTPUT_MODE\n\ncache_dir = CACHE_DIR",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n        os.makedirs(REPORTS_DIR)\nif not os.path.exists(REPORTS_DIR):\n    os.makedirs(REPORTS_DIR)\n    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n    os.makedirs(REPORTS_DIR)",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "processor = BinaryClassificationProcessor()\ntrain_examples = processor.get_train_examples(DATA_DIR)\ntrain_examples_len = len(train_examples)",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "label_list = processor.get_labels() # [0, 1] for binary classification\nnum_labels = len(label_list)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_train_optimization_steps = int(\n    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BertTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-4a888328a234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained model tokenizer (vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}